{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "generative_ai_disabled": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickv779/LADS-Notebooks/blob/main/HW6_Learning_to_Learn_Fall25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "socIlC1y00ug"
      },
      "source": [
        "# Homework 6 (Learning to Learn)\n",
        "# Lin. Alg. for Data Science.\n",
        "## Due date: hopefully a reasonable one, see discord\n",
        "\n",
        "\n",
        "\n",
        "# Instructions:\n",
        "0. Solve the tasks (by writing python code and answering extra questions).\n",
        "1. Prefix the name of the notebook with your name (e.g. My_Name_HW6.ipynb)\n",
        "2. Run all the cells in the notebook, so that all results are visible.\n",
        "\n",
        "3. > Important: on colab create a shared link using the option **\"for anyone with the link\"** and switch permission from **Viewer** to **Editor**, so that it says \"Anyone on the internet with the link can edit\".\n",
        "\n",
        "4. Submit the above link on canvas before the deadline. Do not modify the notebook after that.\n",
        "\n",
        "5. Soon after the final due date, **you will be asked** some brief questions about your solution via a google form I will send you. This is part of the homework assignment!\n",
        "\n",
        "> This time I will also ask you to copy-paste some of your answers from this notebook to the form.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collaboration rules:\n",
        "For this assignment, you can discuss on discord etc. Don't share the entire code, that'd be kind of pointless.\n",
        "\n",
        "> Also **refrain from showing the resulting plots** -- I think it's more rewarding if everyone sees their own output first. But feel free to share your results after everyone is done.\n",
        "\n"
      ],
      "metadata": {
        "id": "AcQ09WkAffWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "In HW4 we lost our battle with the CIFAR dataset. The resulting embedding did not really reflect the correct labels. We concluded that comparing the images using the Euclidean distance was not suffiecient to capture more intricate structures present in these images.\n",
        "\n",
        "Now, we will apply tSNE to the feature vectors output by a Convolutional Neural Network (CNN). This should give us a much better embedding!\n",
        "\n",
        "The purpose of this assignment is to make sure you get some familiarity with keras. Also this is **not a typical scenario** for using neural networks -- but it makes sense to mix and match various tools. After all, a CNN returns a vector for each input image -- and we know many tools that take vectors.\n",
        "\n",
        "We will use CIFAR10 dataset since it has fewer labels.\n",
        "\n",
        "> Note that this is one long analysis, not just separate programming exercises. To get credit, it should be clear you know what you're doing!\n",
        "\n",
        "Since it's the end of the semester I tried to keep it shorter -- and you should be able to reuse a lot of the code and concepts from HW4. So hopefully it'll be quick.\n"
      ],
      "metadata": {
        "id": "sZT47qVrHvsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Context\n",
        "\n",
        "In this assignment we go back to the setup we considered in HW4 -- which different from what we did in HW5, for example. So it's important that you respect the broad setup from HW4."
      ],
      "metadata": {
        "id": "dRmv5sHyWkmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "sdR3qDjwxh32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 0\n",
        "\n",
        "Load the cifar10 data.\n",
        "\n",
        "> Remember to check how to best structure the data for the network you use later!"
      ],
      "metadata": {
        "id": "_vR77AMmGnuT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ctO-MFOxCtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "\n",
        "Cook up a **CNN** which accepts cifar10 images as input.\n",
        "\n",
        "> Remember the class in which we discussed how to create a network to exploit the full potential of CNNs? Use the tricks we did then. It may still take some tuning.\n",
        "\n",
        "> Make sure you don't run out of colab GPU (wink, wink) compute credits. Some people did last year, which kind of means thay misunderstood something..."
      ],
      "metadata": {
        "id": "V2_O6Myum5LM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ozszhEWGh_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2\n",
        "\n",
        "Feed the entire **test data** through your model (or its sub-model), treating it as a **feature extractor**.\n",
        "\n",
        "> Use the **predict** function/method to get your output vectors.\n",
        "\n",
        "> Note that we don't want to work with the final (probabilistic) predictions, we want to use the network as a dimensionality reduction tool. The final\n",
        " (10-dimensional) predictions lose too much information. Make sure the dimension is not too high though."
      ],
      "metadata": {
        "id": "900v20HBniHO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "shspJfygGioo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3\n",
        "\n",
        "Apply tSNE to the resulting vectors as usual.\n",
        "\n",
        "> It's 10k vectors, it should take a minute or so. If it takes longer, maybe you have some more clicking to do?"
      ],
      "metadata": {
        "id": "jbQHxHpnnuoG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D_At8hSYGjOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4\n",
        "\n",
        "Visualize the tSNE output using a 2-dimensional scatterplot, color point depending on the correct label.\n",
        "\n",
        "> The labels[i] corresponds to the i-th class as specified by y_train. In particular it means that images with correct label = 0 are 'airplanes'.\n",
        "\n",
        "> I provide an example code that shows a legend for the labels -- you can replace it with your own if you prefer.\n",
        "\n",
        "> You should get decent grouping which reflects the classes well.\n",
        "\n",
        "> What kind of classes/labels are most often confused? Just have a look."
      ],
      "metadata": {
        "id": "nm-URiDwn3qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I prepared the cifar10 labels for you -- see, I'm not a bad person!\n",
        "labels = np.array('airplanes,cars,birds,cats,deer,dogs,frogs,horses,ships,trucks'.split(','))"
      ],
      "metadata": {
        "id": "utkOzXeCGj0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Here is an example code for plotting the points with labels.\n",
        "Somehow this caused the most problems last year,\n",
        "but that's not my intention.\n",
        "\n",
        "If you know a cleaner way of doing this, please share...\n",
        "'''\n",
        "\n",
        "# example data -- remove and use real data!!!\n",
        "n = 1000\n",
        "x_test = np.random.rand(n, 2)\n",
        "y_test = np.random.randint(0, 10, size=n)\n",
        "\n",
        "np.random.seed(8)\n",
        "c = np.random.rand(10, 3)\n",
        "\n",
        "plt.figure(figsize = (20,20))\n",
        "\n",
        "for lab, col in enumerate(c):\n",
        "    points = x_test[y_test==lab]\n",
        "    plt.scatter(*points.T, color = c[lab] , label = labels[lab])\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "7in-B5rgxUZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5\n",
        "\n",
        "Visualize the tSNE output using a scatterplot, but this time show the image corresponding to each point.\n",
        "\n",
        "> You should see nice structure. It should be clear that -- unlike HW4 -- it's no longer just about color!\n",
        "\n",
        "> Enjoy the view!\n",
        "\n",
        "> This should look much better than what we got from tSNE in HW4. But keep in mind that we needed a powerful network trained on labeled image data. So unlike HW4 it's not really fully unsupervised learning, but rather semi-supervised learning."
      ],
      "metadata": {
        "id": "LIDtX1yPoNQO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nzCP1RbTXa05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6\n",
        "\n",
        "Similarly to what we did in HW4, find (by eyeballing, no code!):\n",
        "- 5 points corresponding to images that show good performance, for example very differently looking images (e.g. different colors) having the same label that got correctly grouped closely together.\n",
        "\n",
        "\n",
        "- 5 points corresponding to images that show bad performance, for images with completely different labels that got grouped together (but shouldn't be).\n",
        "\n",
        "Again, as in HW4, plot the 10 nearest neighbours for each of these 10 points in the low-dimensional space, and the relevant high-dimensional space.\n",
        "\n",
        "> So you should show 200 images in total. I recommend using for loops :)\n",
        "\n",
        "**Q**: Do you notice anything interesting? In particular, does the grouping\n",
        "still rely on similar colors? How good is tSNE at preserving the neighbourhoods in this case? In what situations is there a confusion (images with different labels are placed together). Is this the same for the nearest neighbours in the low and high-dimensional space?\n",
        "\n",
        "**A**:"
      ],
      "metadata": {
        "id": "ooGqqM3eXGZv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m1LL2T6oGkaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus\n",
        "\n",
        "Use a network fine-tuned for classification on CIFAR10 (that's the advanced type of transfer learning discuss in class).\n",
        "\n",
        "* Repeat the 2 tSNE plots using the carefully fine tuned CNN -- you should see even clearer separation into classes.\n",
        "\n",
        "> If you want to fine-tune you will have to carefully unfreeze some of the layers -- there is code for that in the notebook, so that's no secret, but you may need to tune it some more\n",
        "\n",
        "> You should get better results, but originally tSNE worked only on the image data, so it's a little unfair.\n",
        "\n",
        "* Now do the 2 plots using the output of the entire classifier. Do you see any difference? Also: which one would you use in practice to explore a dataset?\n",
        "\n",
        "* For extra points: Anything else you'd like to do? Maybe try how the new(ish) vision transformer would do? Or some other network you used or heard about? Then do it!\n",
        "\n",
        "* You could also try to visualize the final predictions with the small image plot -- that's a different task, but often useful for checking what images are mispredicted, etc. Caveat: these vectors are best measured with the KL divergence, and not the Euclidean distance used by tSNE by default? Anything you can do to remove (or at least alleviate) this mismatch?"
      ],
      "metadata": {
        "id": "xfVgDywhobaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pf9eT__WGl3w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}